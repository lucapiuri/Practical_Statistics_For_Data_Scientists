{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73975f20-0b85-48c2-bd3f-60b543304008",
   "metadata": {},
   "source": [
    "## 174 - Model Selection and Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56bd665d-328a-4263-9c01-fea41776087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "from pygam import LinearGAM, s, l\n",
    "from pygam.datasets import wage\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dmba = Data Mining for Business Analytics\n",
    "from dmba import stepwise_selection\n",
    "from dmba import AIC_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f77e0201-8e99-4d8c-98ef-077dd9b978b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('house_sales.csv', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ddc37be-8dae-4088-a0b4-3a5e32b15af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           AdjSalePrice   R-squared:                       0.595\n",
      "Model:                            OLS   Adj. R-squared:                  0.594\n",
      "Method:                 Least Squares   F-statistic:                     2771.\n",
      "Date:                Tue, 14 Jan 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:12:41   Log-Likelihood:            -3.1375e+05\n",
      "No. Observations:               22687   AIC:                         6.275e+05\n",
      "Df Residuals:                   22674   BIC:                         6.276e+05\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "SqFtTotLiving                198.6364      4.234     46.920      0.000     190.338     206.934\n",
      "SqFtLot                        0.0771      0.058      1.330      0.184      -0.037       0.191\n",
      "Bathrooms                   4.286e+04   3808.114     11.255      0.000    3.54e+04    5.03e+04\n",
      "Bedrooms                   -5.187e+04   2396.904    -21.638      0.000   -5.66e+04   -4.72e+04\n",
      "BldgGrade                   1.373e+05   2441.242     56.228      0.000    1.32e+05    1.42e+05\n",
      "NbrLivingUnits              5723.8438   1.76e+04      0.326      0.744   -2.87e+04    4.01e+04\n",
      "SqFtFinBasement                7.0611      4.627      1.526      0.127      -2.009      16.131\n",
      "YrBuilt                    -3574.2210     77.228    -46.282      0.000   -3725.593   -3422.849\n",
      "YrRenovated                   -2.5311      3.924     -0.645      0.519     -10.222       5.160\n",
      "NewConstruction            -2489.1122   5936.692     -0.419      0.675   -1.41e+04    9147.211\n",
      "PropertyType_Single Family  2.997e+04   2.61e+04      1.149      0.251   -2.12e+04    8.11e+04\n",
      "PropertyType_Townhouse      9.286e+04    2.7e+04      3.438      0.001    3.99e+04    1.46e+05\n",
      "const                       6.182e+06   1.55e+05     39.902      0.000    5.88e+06    6.49e+06\n",
      "==============================================================================\n",
      "Omnibus:                    31006.128   Durbin-Watson:                   1.393\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         26251977.078\n",
      "Skew:                           7.427   Prob(JB):                         0.00\n",
      "Kurtosis:                     168.984   Cond. No.                     2.98e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.98e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Here we are dealing with both numerical and categorical predictors.\n",
    "# we need to convert the categorical and boolean variables into numbers\n",
    "\n",
    "outcome = 'AdjSalePrice'\n",
    "predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n",
    "              'BldgGrade', 'PropertyType', 'NbrLivingUnits',\n",
    "              'SqFtFinBasement', 'YrBuilt', 'YrRenovated', \n",
    "              'NewConstruction']\n",
    "\n",
    "    # Some of these, like 'PropertyType', are categorical - they don't have numerical values but rather categories, in this case:\n",
    "    # 'Multiplex', 'Single Family', 'Townhouse'\n",
    "    # We can't use categorical variables directly in regression - they need to be converted to numbers first.\n",
    "    # We do this with \"pd.get_dummies()\"\n",
    "\n",
    "    # This function performs what's called \"one-hot encoding\". \n",
    "    # 'PropertyType' has three categories: 'Multiplex', 'Single Family', 'Townhouse'. \n",
    "    # The function creates new binary columns (0s and 1s) for each category except one. We drop one category (drop_first=True) \n",
    "    # to avoid what's called the \"dummy variable trap\" / multicollinearity,\n",
    "    # a situation where perfect correlation between our predictors would make the math impossible.\n",
    "\n",
    "X = pd.get_dummies(house[predictors], drop_first=True, dtype=int)\n",
    "\n",
    "\n",
    "    # The goal of this list comprehension is to ensure our NewConstruction column contains only 1s and 0s. \n",
    "    # We have 'True' and 'False' values, that we want to turn into 0s and 1s\n",
    "\n",
    "    # that below is the same as (longer, clearer, step-by-step version):\n",
    "    # new_values = []  # Create an empty list to store our converted values\n",
    "    # for nc in X['NewConstruction']:  # Loop through each value in the NewConstruction column\n",
    "    # if nc:  # If the value is \"truthy\"\n",
    "    #    new_value = 1\n",
    "    # else:  # If the value is \"falsy\"\n",
    "    #    new_value = 0\n",
    "    # new_values.append(new_value)  # Add our converted value to the list\n",
    "    # # Replace the old column with our new values:\n",
    "    # X['NewConstruction'] = new_values\n",
    "\n",
    "    #  It says \"Create a list. For each value (nc) in X['NewConstruction'], include 1 if the value is truthy, otherwise include 0.\"\n",
    "\n",
    "    # The \"if nc else 0\" part is using Python's \"truthiness\" concept:\n",
    "    #    - Values like True, 'Y', 1, and non-empty strings are considered \"truthy\" and will become 1\n",
    "    #    - Values like False, 'N', 0, None, and empty strings are considered \"falsy\" and will become 0\n",
    "\n",
    "X['NewConstruction'] = [1 if nc else 0 for nc in X['NewConstruction']]\n",
    "\n",
    "\n",
    "    # We create and fit our model:\n",
    "    # The basic syntax is: sm.OLS(dependent_variable, independent_variables)\n",
    "\n",
    "    # The first argument house[outcome] is our dependent variable (what we're trying to predict). \n",
    "    # The sale price of houses. Think of this as the \"answer\" we're trying to predict.\n",
    "\n",
    "    # The second argument X.assign(const=1) is more complex. \n",
    "    # It contains all our independent variables (predictors) plus a special constant term:\n",
    "    #    - X is our DataFrame containing all our predictor variables\n",
    "    #    - .assign(const=1) adds a new column called 'const' filled with 1s\n",
    "\n",
    "    # Why do we need this column of 1s? \n",
    "    # It's related to the equation of a line: y = mx + b. \n",
    "    # In regression, that 'b' (the intercept) needs its own predictor column of all 1s to be estimated properly. \n",
    "\n",
    "    # Why 1 specifically?\n",
    "    # If we use a column of 1s, when all other x's are 0, \n",
    "    # y = β₀ * 1 = β₀\n",
    "    # So β₀ directly represents our intercept\n",
    "    # Using 1s is the convention because it gives us the intercept directly, it's clean and simple\n",
    "\n",
    "    # When we call this function, statsmodels creates an OLS object that's ready to be fitted. \n",
    "    # It's like setting up the equation y = mx + b, but we haven't solved for m and b yet. \n",
    "    # That solving happens when we call .fit() later.\n",
    "\n",
    "house_full = sm.OLS(house[outcome], X.assign(const=1))\n",
    "\n",
    "\n",
    "    # We fit the model:\n",
    "results = house_full.fit()\n",
    "\n",
    "\n",
    "    # We print the summary:\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "    # When we print the summary now, we'll see coefficients for all our numeric variables plus separate coefficients \n",
    "    # for each category of our categorical variables (except the dropped reference categories). \n",
    "    # Each coefficient tells us how much the house price changes when:\n",
    "    #    - For numeric variables: the variable increases by one unit\n",
    "    #    - For categorical variables: the house is in that category compared to the reference category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c158327-c555-44d2-aecc-5f0a0ff079d3",
   "metadata": {},
   "source": [
    "### Using the dmba package\n",
    "\n",
    "\n",
    "\n",
    "We can use the stepwise_selection method from the dmba package.\n",
    "\n",
    "scikit-learn has no implementation for stepwise regression. \n",
    "The functions \"stepwise_selection\", \"forward_selection\", and \"backward_elimination\" have been implemented in the dmba package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28528153-b366-4110-9cee-7c4f5604a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: SqFtTotLiving, SqFtLot, Bathrooms, Bedrooms, BldgGrade, NbrLivingUnits, SqFtFinBasement, YrBuilt, YrRenovated, NewConstruction, PropertyType_Single Family, PropertyType_Townhouse\n",
      "Start: score=647988.32, constant\n",
      "Step: score=633013.35, add SqFtTotLiving\n",
      "Step: score=630793.74, add BldgGrade\n",
      "Step: score=628230.29, add YrBuilt\n",
      "Step: score=627784.16, add Bedrooms\n",
      "Step: score=627602.21, add Bathrooms\n",
      "Step: score=627525.65, add PropertyType_Townhouse\n",
      "Step: score=627525.08, add SqFtFinBasement\n",
      "Step: score=627524.98, add PropertyType_Single Family\n",
      "Step: score=627524.98, unchanged None\n",
      "\n",
      "Intercept: 6178645.017\n",
      "Coefficients:\n",
      " SqFtTotLiving: 199.27755304201884\n",
      " BldgGrade: 137159.56022619773\n",
      " YrBuilt: -3565.4249392492993\n",
      " Bedrooms: -51947.38367361318\n",
      " Bathrooms: 42396.164527717796\n",
      " PropertyType_Townhouse: 84479.16203300405\n",
      " SqFtFinBasement: 7.046974967553979\n",
      " PropertyType_Single Family: 22912.055187017682\n"
     ]
    }
   ],
   "source": [
    "# Using the DMBA Package (Data Mining for Business Analytics)\n",
    "    # Stepwise selection method to build a regression model, leveraging the DMBA package\n",
    "\n",
    "    # Stepwise selection is a technique to iteratively add or remove predictors (variables) \n",
    "    # from a regression model based on their contribution to the model's performance, \n",
    "    # measured using a criterion like AIC (Akaike Information Criterion).\n",
    "\n",
    "\n",
    "    # Defining the target variable (dependent variable) for the regression\n",
    "    \n",
    "outcome = 'AdjSalePrice'\n",
    "y = house[outcome]\n",
    "\n",
    "\n",
    "    # Define a function that returns a fitted model for a given set of variables, the train_model function\n",
    "    # It trains a linear regression model using the provided predictors (variables).\n",
    "    # Returns a  fitted LinearRegression model, or None if no variables are provided.\n",
    "def train_model(variables):\n",
    "    if len(variables) == 0:\n",
    "        return None\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[variables], y)\n",
    "    return model\n",
    "\n",
    "\n",
    "    # Define the score_model Function\n",
    "    # Define a function that returns a score for a given model and set of variables. \n",
    "    # In this case, we use the AIC_score implemented in the dmba package.\n",
    "    # The purpose is to valuates the performance of a model using AIC (Akaike Information Criterion).\n",
    "    # Parameters:\n",
    "    #    - model: A fitted regression model\n",
    "    #    - variables: A list of column names used as predictors in the model\n",
    "    # Returns the AIC score for the model.\n",
    "    \n",
    "def score_model(model, variables):\n",
    "\n",
    "\n",
    "    # It computes the Akaike Information Criterion (AIC) score for a baseline model that predicts the mean of y for all observations. \n",
    "    # This is used as a reference to compare other models in the stepwise selection process.\n",
    "\n",
    "    # How AIC Works\n",
    "    # AIC measures the tradeoff between model complexity and goodness of fit.\n",
    "    # Lower AIC values indicate a better model.\n",
    "    \n",
    "    # AIC parameters:\n",
    "    #    - y: the true (observed) values of the dependent variable. Used to compute the residual errors of the model.\n",
    "    #    - y_pred: The predicted values from the model for the dependent variable.\n",
    "    #      In the baseline model (predicting the mean of y), y_pred is a constant array equal to [y.mean()] * len(y).\n",
    "    #    - model: The fitted model object. \n",
    "    #      This provides necessary details like the residual sum of squares (RSS) or log-likelihood of the model.\n",
    "    #    - df: The degrees of freedom for the model. \n",
    "    #      This is typically the number of parameters in the model, including the intercept.\n",
    "    # AIC = 2k − 2 ln (L)\n",
    "    # L: The likelihood of the model given the data (based on the residual errors). (The residuals are calculated as y− ypred)\n",
    "    \n",
    "\n",
    "    # Computes the AIC score for a baseline model that predicts the mean value of y for all data points.\n",
    "    # This baseline AIC score serves as the \"starting point\" or reference for the stepwise selection process.\n",
    "\n",
    "    # It's important because if no predictors are selected (len(variables) == 0), \n",
    "    # this line ensures the stepwise selection process can still evaluate and compare the baseline model.\n",
    "    # Models with predictors will be scored against this baseline to determine whether they improve model performance.\n",
    "        \n",
    "    # Parameters:\n",
    "    #     - y : he true values of the dependent (target) variable.\n",
    "    #     - [y.mean()] * len(y) : creates a list where every element is the mean of y. \n",
    "    #       Its length matches the number of observations in y.\n",
    "    #       This represents the predictions of a baseline model that always predicts the mean value of y.\n",
    "    #     - model: Refers to the model object. It can be None in this case, \n",
    "    #       as the function is computing AIC for the baseline (mean-only) model. \n",
    "    #     - df=1: The degrees of freedom for the baseline model. Since it predicts a single value (the mean), \n",
    "    #       it has only 1 degree of freedom.\n",
    "        \n",
    "    if len(variables) == 0:\n",
    "        return AIC_score(y, [y.mean()] * len(y), model, df=1)\n",
    "        \n",
    "    return AIC_score(y, model.predict(X[variables]), model)\n",
    "\n",
    "\n",
    "    # Perform Stepwise Selection\n",
    "    # We use stepwise selection to find the best combination of predictors for the regression model.\n",
    "    # Parameters:\n",
    "    #    - X.columns: The initial set of all predictors (independent variables).\n",
    "    #    - train_model: The function to train the model.\n",
    "    #    - score_model: The function to score the model.\n",
    "    #    - verbose=True: Prints the selection process (which variables are added/removed).\n",
    "\n",
    "    # Results\n",
    "    #    - best_model: The final regression model\n",
    "    #    - best_variables: The set of predictors selected by stepwise selection\n",
    "\n",
    "best_model, best_variables = stepwise_selection(X.columns, train_model, score_model, \n",
    "                                                verbose=True)\n",
    "    # Print values\n",
    "print()\n",
    "    # Intercept: The value of the dependent variable when all predictors are 0:\n",
    "\n",
    "print(f'Intercept: {best_model.intercept_:.3f}')\n",
    "\n",
    "    # Coefficients: The relationship between each selected predictor and the target variable:\n",
    "print('Coefficients:')\n",
    "for name, coef in zip(best_variables, best_model.coef_):\n",
    "    print(f' {name}: {coef}')\n",
    "\n",
    "# The code uses stepwise selection to find the best predictors for a regression model.\n",
    "# It trains and evaluates models using AIC as the criterion.\n",
    "# Finally, it prints the intercept and coefficients of the best model\n",
    "# X is a DataFrame of predictors, and y is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587c97c-6211-4c4c-bd47-8ff28cafd065",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c966bd-5f16-40e5-a27b-58f9449d7055",
   "metadata": {},
   "source": [
    "The function chose a model in which several variables were dropped from house_full: SqFtLot, NbrLivingUnits, YrRenovated, and NewConstruction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
