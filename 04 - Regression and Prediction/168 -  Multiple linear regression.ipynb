{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0da598-c947-4338-84c7-a1041e0da29a",
   "metadata": {},
   "source": [
    "### 168 - Multiple linear regression\n",
    "\n",
    "A linear regression tries to find a linear relationship between multiple input variables (predictors) and an output variable (outcome) by fitting a line (or hyperplane in multiple dimensions) that best matches the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55532989-024c-4426-be91-8c2e4fb1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "from pygam import LinearGAM, s, l\n",
    "from pygam.datasets import wage\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f365f31-a385-446b-b6fa-f1ca3117ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ccfa8c-5d47-4e4b-b450-d4061cca9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AdjSalePrice  SqFtTotLiving  SqFtLot  Bathrooms  Bedrooms  BldgGrade\n",
      "1      300805.0           2400     9373       3.00         6          7\n",
      "2     1076162.0           3764    20156       3.75         4         10\n",
      "3      761805.0           2060    26036       1.75         4          8\n",
      "4      442065.0           3200     8618       3.75         5          7\n",
      "5      297065.0           1720     8620       1.75         4          7\n"
     ]
    }
   ],
   "source": [
    "# We define a subset of colums that we will consider from the csv file\n",
    "subset = ['AdjSalePrice', 'SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n",
    "          'Bedrooms', 'BldgGrade']\n",
    "\n",
    "\n",
    "\n",
    "house = pd.read_csv('house_sales.csv', sep='\\t')\n",
    "print(house[subset].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "019f7213-14ab-4bd3-bf92-c50dfee30c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -521871.368\n",
      "Coefficients:\n",
      " SqFtTotLiving: 228.83060360240796\n",
      " SqFtLot: -0.06046682065307607\n",
      " Bathrooms: -19442.840398321056\n",
      " Bedrooms: -47769.95518521438\n",
      " BldgGrade: 106106.96307898083\n"
     ]
    }
   ],
   "source": [
    "# Defining the variables:\n",
    "\n",
    "    # We definine which features of the houses will be used to predict the price:\n",
    "predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n",
    "              'Bedrooms', 'BldgGrade']\n",
    "\n",
    "    # We set the target variable (the adjusted sale price of houses, which we're trying to predict):\n",
    "outcome = 'AdjSalePrice'\n",
    "\n",
    "\n",
    "# Regression:\n",
    "\n",
    "    # A LinearRegression object is instantiated from scikit-learn:\n",
    "    # We're creating an instance of the LinearRegression class. At this point\n",
    "    # it's like an empty container waiting to learn from data. \n",
    "    # It's like a brand new calculator that hasn't performed any calculations yet.\n",
    "\n",
    "house_lm = LinearRegression()\n",
    "\n",
    "\n",
    "    # The fit() method finds the optimal coefficients using the training data. \n",
    "    # After we call the the fit() method, the coefficients get created and stored inside the object as \"attributes\"\n",
    "    #     - \"coef_\" stores the coefficients (the slopes)\n",
    "    #     - \"intercept_\" stores the intercept (the y-intercept)\n",
    "\n",
    "    # The fit method takes your input data (X) and target values (y)\n",
    "    # It performs the mathematical calculations to find the optimal coefficients using ordinary least squares\n",
    "    # Once it finds these values, it stores them as attributes inside the object\n",
    "    # These stored values can then be used later for predictions or analysis\n",
    "\n",
    "    #  This pattern - creating an empty object and then filling it with learned parameters through a fit method - \n",
    "    # is common in scikit-learn. It's called the \"estimator API\" pattern.\n",
    "\n",
    "house_lm.fit(house[predictors], house[outcome])\n",
    "\n",
    "\n",
    "    # We print the results:\n",
    "    \n",
    "print(f'Intercept: {house_lm.intercept_:.3f}')\n",
    "print('Coefficients:')\n",
    "\n",
    "    # Zip: The core purpose here is to pair up two related pieces of information: \n",
    "    # the predictor names and their corresponding coefficients.\n",
    "    # Like a zipper on a jacket - it takes two separate rows of teeth and joins them together, pair by pair. \n",
    "    # It takes multiple iterables (like lists) and creates pairs (or tuples) of items from each list, matching them by position.\n",
    "    # Without zip, we'd have to access these values using indices, which would be more cumbersome\n",
    "    # Using zip makes the code both more elegant and safer\n",
    "    # This pairing is particularly valuable here because these values are inherently related - \n",
    "    # each coefficient corresponds to a specific predictor variable. \n",
    "    # Using zip maintains this relationship explicitly in our code, making it more readable, \n",
    "    # less prone to errors (we don't risk mixing up indices) and \n",
    "    # More maintainable (if we add or remove predictors, the code still works as long as the lists match in length)\n",
    "\n",
    "for name, coef in zip(predictors, house_lm.coef_):\n",
    "    print(f' {name}: {coef}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a97ee4-5deb-45f2-9345-e923e5a20f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.28830604e+02, -6.04668207e-02, -1.94428404e+04, -4.77699552e+04,\n",
       "        1.06106963e+05])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb65d6-a8c7-4461-b5dd-585b42fcf55c",
   "metadata": {},
   "source": [
    "## Assessing the Model\n",
    "\n",
    "Scikit-learn provides a number of metrics to determine the quality of a model. Here we use the \"r2_score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bb0b94d-78c4-4e26-9fc1-2c3ce88740a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 261220\n",
      "r2: 0.5406\n"
     ]
    }
   ],
   "source": [
    "# SCIKIT-LEARN:\n",
    "# This evaluates how well our linear regression model performs\n",
    "\n",
    "    # After we trained our model with fit(), we can use it to make predictions. \n",
    "    # The predict() method takes our predictor variables and applies the coefficients we learned to generate predicted house prices. \n",
    "    # These predictions are called \"fitted values\" because they represent the prices that \"fit\" our model. \n",
    "    # Think of it like drawing a line through a scatter plot - the fitted values are the points that lie exactly on that line.\n",
    "\n",
    "fitted = house_lm.predict(house[predictors])\n",
    "\n",
    "\n",
    "    # RMSE (Root Mean Square Error) tells us how far off our predictions are from the actual house prices, on average. \n",
    "    # Here's how it works:\n",
    "    #    - Take each prediction and subtract the actual price (house[outcome] - fitted)\n",
    "    #    - Square these differences (to make negatives positive and emphasize big errors)\n",
    "    #    - Calculate the mean of these squared differences (mean_squared_error)\n",
    "    #    - Take the square root (np.sqrt) to get back to the original units (dollars)\n",
    "    # \"mean_squared_error\" arguments: Ground truth (correct) target values, Estimated target values.\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(house[outcome], fitted))\n",
    "\n",
    "\n",
    "    # R² (R-squared) tells us what proportion of the variation in house prices our model explains (ranges from 0 to 1):\n",
    "    # \"r2_score\" arguments: (ground truth (correct) target values, Estimated target values)\n",
    "\n",
    "r2 = r2_score(house[outcome], fitted)\n",
    "\n",
    "\n",
    "    # We print the values:\n",
    "print(f'RMSE: {RMSE:.0f}')\n",
    "print(f'r2: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488796a4-f8c9-412c-9e2b-bd05e36f3a61",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "While scikit-learn provides a variety of different metrics, statsmodels provides a more in-depth analysis of the linear regression model. This package has two different ways of specifying the model, one that is similar to scikit-learn and one that allows specifying R-style formulas. \n",
    "\n",
    "Here we use the first approach. As statsmodels doesn't add an intercept automaticaly, we need to add a constant column with value 1 to the predictors. We can use the pandas method assign for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70458128-8b78-4fa2-b797-d84299684496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           AdjSalePrice   R-squared:                       0.541\n",
      "Model:                            OLS   Adj. R-squared:                  0.540\n",
      "Method:                 Least Squares   F-statistic:                     5338.\n",
      "Date:                Tue, 14 Jan 2025   Prob (F-statistic):               0.00\n",
      "Time:                        13:06:24   Log-Likelihood:            -3.1517e+05\n",
      "No. Observations:               22687   AIC:                         6.304e+05\n",
      "Df Residuals:                   22681   BIC:                         6.304e+05\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "SqFtTotLiving   228.8306      3.899     58.694      0.000     221.189     236.472\n",
      "SqFtLot          -0.0605      0.061     -0.988      0.323      -0.180       0.059\n",
      "Bathrooms     -1.944e+04   3625.388     -5.363      0.000   -2.65e+04   -1.23e+04\n",
      "Bedrooms      -4.777e+04   2489.732    -19.187      0.000   -5.27e+04   -4.29e+04\n",
      "BldgGrade      1.061e+05   2396.445     44.277      0.000    1.01e+05    1.11e+05\n",
      "const         -5.219e+05   1.57e+04    -33.342      0.000   -5.53e+05   -4.91e+05\n",
      "==============================================================================\n",
      "Omnibus:                    29676.557   Durbin-Watson:                   1.247\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         19390738.346\n",
      "Skew:                           6.889   Prob(JB):                         0.00\n",
      "Kurtosis:                     145.559   Cond. No.                     2.86e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.86e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# STATSMODEL:\n",
    "# This evaluates how well our linear regression model performs (with statsmodel this time)\n",
    "\n",
    "    # The sm.OLS() function creates an Ordinary Least Squares (OLS) regression model. \n",
    "    # Unlike scikit-learn, statsmodels doesn't automatically include an intercept term \n",
    "    # that's why we use .assign(const=1) to add a column of 1's to our predictors. \n",
    "    # This constant term allows the model to fit an intercept.\n",
    "\n",
    "    # With the following line, we are creating an object, specifically an OLS (Ordinary Least Squares) object. \n",
    "    # However, this line is doing more than just creating an empty container,\n",
    "    # it's actually setting up all the data and mathematical structures needed for the regression.\n",
    "\n",
    "    # With scikit-learn's LinearRegression(), we just create an empty object,\n",
    "    # With statsmodels' OLS(), the OLS constructor immediately prepares several key mathematical components, technical stuff like:\n",
    "    #    - Preparing the Design Matrix (X) (The constructor takes our predictor variables and transforms them into \n",
    "    #      a matrix format suitable for linear algebra operations. \n",
    "    #      Each column represents a predictor variable, and each row represents an observation)\n",
    "    #    - Preparing the Response Vector (y) (It's our outcome variable (house prices) formatted as a vector that will be used \n",
    "    #      in the matrix calculations.\n",
    "    #    - Preparing matrix Properties (The constructor calculates and stores important properties of \n",
    "    #      these matrices that will be needed later).\n",
    "    # The constructor also sets up the mathematical framework for what will happen during fitting.\n",
    "    # While the OLS object doesn't perform these calculations yet (that happens in fit()), it has everything organized and ready to go. \n",
    "    # It's like having all your ingredients measured and your pans ready before you start cooking.\n",
    "\n",
    "    # This preparation is different from scikit-learn's approach, which is why we need to explicitly add the constant term. \n",
    "    # Statsmodels is designed to give us more control and visibility into the statistical processes, \n",
    "    # while scikit-learn prioritizes ease of use.\n",
    "\n",
    "model = sm.OLS(house[outcome], house[predictors].assign(const=1))\n",
    "\n",
    "\n",
    "# we fit the model:\n",
    "    # This line performs the actual regression calculations, similar to what we did with scikit-learn. \n",
    "    # However, statsmodels stores much more detailed statistical information about the regression.\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "\n",
    "    # Prints a comprehensive statistical report\n",
    "print(results.summary())\n",
    "\n",
    "    # This summary provides several important pieces of information:\n",
    "    # Overall model statistics:\n",
    "    #    - R-squared and Adjusted R-squared tell us how well our model fits\n",
    "    #      (The adjusted R-squared, which adjusts for the degrees of freedom, \n",
    "    #      effectively penalizing the addition of more predictors to a model)\n",
    "    #    - F-statistic tests if our model is better than just using the mean\n",
    "    #    - AIC and BIC help us compare different possible models\n",
    "\n",
    "    #For each predictor:\n",
    "    #    - Coefficient (coef): The estimated effect on house price\n",
    "    #    - Standard error (std err): The uncertainty in our coefficient estimate\n",
    "    #    - t-statistic (t): Tests if the coefficient is significantly different from zero\n",
    "    #    - P-value (P>|t|): The probability of seeing such a coefficient by chance\n",
    "    #    - Confidence intervals [0.025 0.975]: The range where we believe the true coefficient lies\n",
    "\n",
    "    # The t-statistic — and its mirror image, the p-value — measures the extent to which a coefficient is \n",
    "    # “statistically significant”—that is, outside the range of what a random chance arrangement of predictor \n",
    "    # and target variable might produce. \n",
    "    # The higher the t-statistic (and the lower the p-value), the more significant the predictor. \n",
    "    \n",
    "    # [0.025, 0.975]: These numbers represent the lower bound (2.5th percentile) and upper bound (97.5th percentile) \n",
    "    # of the confidence interval. This corresponds to the 95% confidence level (100% - 2.5% - 2.5%).\n",
    "    # Interpretation:\n",
    "    # If the confidence interval for a coefficient does not include 0, it indicates that the coefficient \n",
    "    # is statistically significant at the 95% confidence level.\n",
    "    \n",
    "    # (For example, if the confidence interval for a variable is [1.5, 3.2], it means the true coefficient \n",
    "    # is likely between 1.5 and 3.2, with 95% confidence)\n",
    "\n",
    "    # DATA SCIENCE NOTE:\n",
    "    # p-value (Pr(>|t|) and F-statistic: Data scientists do not generally get too involved with the interpretation of these statistics, \n",
    "    # nor with the issue of statistical significance. \n",
    "    # Data scientists primarily focus on the t-statistic as a useful guide for whether to include a predictor in a model or not. \n",
    "    # High t-statistics (which go with p-values near 0) indicate a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa33669-7bd9-4373-8402-66f30a1af86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
